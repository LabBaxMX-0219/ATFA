{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8205f5",
   "metadata": {
    "papermill": {
     "duration": 0.004207,
     "end_time": "2023-07-01T03:46:40.276906",
     "exception": false,
     "start_time": "2023-07-01T03:46:40.272699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(sweep using wandb)[https://docs.wandb.ai/tutorials/sweeps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976665f8",
   "metadata": {
    "papermill": {
     "duration": 0.00356,
     "end_time": "2023-07-01T03:46:40.284430",
     "exception": false,
     "start_time": "2023-07-01T03:46:40.280870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model & Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c10e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T03:46:40.294661Z",
     "iopub.status.busy": "2023-07-01T03:46:40.293933Z",
     "iopub.status.idle": "2023-07-01T03:46:43.935885Z",
     "shell.execute_reply": "2023-07-01T03:46:43.934919Z"
    },
    "papermill": {
     "duration": 3.650138,
     "end_time": "2023-07-01T03:46:43.938524",
     "exception": false,
     "start_time": "2023-07-01T03:46:40.288386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "from torch.functional import tensordot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(CNN, self).__init__()\n",
    "        self.modality_num = configs.modality_nums  # 模态数\n",
    "        configs = configs.model_configs['CNN']\n",
    "\n",
    "        self.conv1_blocks = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(configs.input_channels, configs.mid_channels, kernel_size=configs.kernel_size,\n",
    "                      stride=configs.stride, bias=False, padding=(configs.kernel_size // 2)),\n",
    "            nn.BatchNorm1d(configs.mid_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(configs.dropout)\n",
    "        ) for i in range(self.modality_num)])\n",
    "\n",
    "        self.conv2_blocks = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(configs.mid_channels, configs.mid_channels * 2,\n",
    "                      kernel_size=8, stride=1, bias=False, padding=4),\n",
    "            nn.BatchNorm1d(configs.mid_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        ) for i in range(self.modality_num)])\n",
    "\n",
    "        self.conv3_blocks = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(configs.mid_channels * 2, configs.final_out_channels, kernel_size=8, stride=1, bias=False,\n",
    "                      padding=4),\n",
    "            nn.BatchNorm1d(configs.final_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=1),\n",
    "        ) for i in range(self.modality_num)])\n",
    "\n",
    "        self.adaptive_pools = nn.ModuleList([nn.AdaptiveAvgPool1d(\n",
    "            configs.features_len) for i in range(self.modality_num)])\n",
    "\n",
    "    def forward(self, k_x_in):\n",
    "        '''x: [k*(N, 3, 128)]'''\n",
    "        k_out = []\n",
    "        for modality_idx in range(self.modality_num):\n",
    "            x = k_x_in[modality_idx]\n",
    "            x = self.conv1_blocks[modality_idx](x)\n",
    "            x = self.conv2_blocks[modality_idx](x)\n",
    "            x = self.conv3_blocks[modality_idx](x)\n",
    "            x = self.adaptive_pools[modality_idx](x)\n",
    "            x_flat = x.reshape(x.shape[0], 1, -1)\n",
    "            # print(x_flat.shape)  # (N, 1, 128)\n",
    "            k_out.append(x_flat)\n",
    "\n",
    "        k_out = torch.cat(k_out, dim=1)\n",
    "        # print(k_out.shape)  # (N, k, 128)\n",
    "        return k_out\n",
    "\n",
    "\n",
    "class c_fusion(nn.Module):\n",
    "    def __init__(self, configs) -> None:\n",
    "        super(c_fusion, self).__init__()\n",
    "        self.modality_num = configs.modality_nums\n",
    "        self.fusion_cfg = configs.model_configs['fusion']\n",
    "        self.d_W1 = nn.Parameter(torch.normal(\n",
    "            mean=0, std=0.1, size=[self.fusion_cfg.final_out_channels, 1], requires_grad=True))\n",
    "        self.d_b1 = nn.Parameter(torch.normal(mean=0, std=0.1, size=[\n",
    "                                 1, self.modality_num, 1]), requires_grad=True)  # (1, k, 1)\n",
    "        self.d_w1 = nn.Parameter(torch.normal(\n",
    "            mean=0, std=0.1, size=[self.modality_num, 1]), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''x: (N, k, len)'''\n",
    "        # print(f\"attn x_unsqueeze: {x.shape}\")  # (N, k, 128)\n",
    "\n",
    "        MLP_input = tensordot(x, self.d_W1, dims=1)\n",
    "        MLP_input += self.d_b1\n",
    "        # print(f\"MLP input: {MLP_input.shape}\")\n",
    "        miu = torch.tanh(MLP_input)  # [BN, k, 1]\n",
    "        softmax_input = tensordot(miu, self.d_w1, dims=1)\n",
    "        # print(f\"softmax_input: {softmax_input.shape}\")\n",
    "        alpha = F.softmax(softmax_input, dim=1)  # [BN, k, 1]\n",
    "        f_per_sensor = alpha * x  # [BN, 1, 64]\n",
    "        # print(f\"f_modality: {f_per_sensor.shape}\")\n",
    "        out = torch.sum(f_per_sensor, dim=1)  # [BN, len]\n",
    "        # print(f\"c_fusion: {out.shape}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class FE(nn.Module):\n",
    "    def __init__(self, configs) -> None:\n",
    "        '''input:\n",
    "                x_k_t: [k*(N, 3, 128)]\n",
    "                x_k_f: [k*(N, 3, 128)]\n",
    "            return:\n",
    "                merge_tf: (N, 2*128)'''\n",
    "        super(FE, self).__init__()\n",
    "        self.cnn_t = CNN(configs=configs)\n",
    "        self.fusion_t = c_fusion(configs=configs)\n",
    "        self.cnn_f = CNN(configs=configs)\n",
    "        self.fusion_f = c_fusion(configs=configs)\n",
    "\n",
    "    def forward(self, x_k_t, x_k_f):\n",
    "        x_k_t = self.cnn_t(x_k_t)\n",
    "        x_k_f = self.cnn_f(x_k_f)  # (N, 2, 128)\n",
    "        # print(x_k_t.shape)  # (N, 2, 128)\n",
    "\n",
    "        merge_t = self.fusion_t(x_k_t)  # (N, 128)\n",
    "        merge_f = self.fusion_f(x_k_f)\n",
    "        # print(merge_f.shape)  # (N, 128)\n",
    "\n",
    "        merge_tf = torch.cat([merge_t, merge_f], dim=1)\n",
    "        # print(merge_tf.shape)  # (N, 256)\n",
    "        return merge_tf\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Classifier, self).__init__()\n",
    "        num_classes = configs.num_classes\n",
    "        configs = configs.model_configs['Classifier']\n",
    "        model_output_dim = configs.features_len\n",
    "        self.hidden_dim = configs.hidden_dim\n",
    "        self.logits = nn.Sequential(\n",
    "            nn.Linear(model_output_dim *\n",
    "                      configs.final_out_channels * 2, self.hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, num_classes),\n",
    "            nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        '''x: (N, 128*2)'''\n",
    "        predictions = self.logits(x_in)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator model for source domain.\"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        \"\"\"Init discriminator.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        configs = configs.model_configs['Discriminator']\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(configs.features_len *\n",
    "                      configs.final_out_channels * 2, configs.disc_hid_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configs.disc_hid_dim*2, configs.disc_hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configs.disc_hid_dim, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward the discriminator.\n",
    "            x: (N, k*len)\"\"\"\n",
    "        out = self.layer(input)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "\n",
    "class ATFA(nn.Module):\n",
    "    \"\"\"\n",
    "    CoDATS: https://arxiv.org/pdf/2005.10996.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs, hparams, device):\n",
    "        super(ATFA, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.channel_num = configs.channel_nums  # 不同模态的通道数e.g. [3,3,1]\n",
    "        self.feature_extractor = FE(configs=configs)\n",
    "        self.classifier_tf = Classifier(configs)\n",
    "        self.domain_discriminator_tf = Discriminator(configs)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            list(self.feature_extractor.parameters()) +\n",
    "            list(self.classifier_tf.parameters()),\n",
    "            lr=hparams[\"learning_rate\"],\n",
    "            weight_decay=hparams[\"weight_decay\"], betas=(0.5, 0.99)\n",
    "        )\n",
    "        self.optimizer_disc = torch.optim.Adam(\n",
    "            self.domain_discriminator_tf.parameters(),\n",
    "            lr=hparams[\"learning_rate\"],\n",
    "            weight_decay=hparams[\"weight_decay\"], betas=(0.5, 0.99)\n",
    "        )\n",
    "        self.hparams = hparams\n",
    "        self.device = device\n",
    "\n",
    "    def get_f_x(self, x):\n",
    "        x_f = torch.fft.rfft(x)\n",
    "        return x_f.real\n",
    "\n",
    "    def update(self, src_x, src_y, trg_x, step, epoch, len_dataloader):\n",
    "        '''x: (N, 6, 128)\n",
    "            y: (N,)'''\n",
    "        p = float(step + epoch * len_dataloader) / \\\n",
    "            self.hparams[\"num_epochs\"] + 1 / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        # split_modality_x\n",
    "        modality_src_x_t = torch.split(\n",
    "            src_x, split_size_or_sections=self.channel_num, dim=1)\n",
    "        modality_trg_x_t = torch.split(\n",
    "            trg_x, split_size_or_sections=self.channel_num, dim=1)\n",
    "\n",
    "        # get t/f x\n",
    "        # src_x_t = src_x\n",
    "        modality_src_x_f = [self.get_f_x(src_x_t)\n",
    "                            for src_x_t in modality_src_x_t]\n",
    "        # trg_x_t = trg_x\n",
    "        modality_trg_x_f = [self.get_f_x(trg_x_t)\n",
    "                            for trg_x_t in modality_trg_x_t]\n",
    "\n",
    "        # zero grad\n",
    "        self.optimizer.zero_grad()\n",
    "        self.optimizer_disc.zero_grad()\n",
    "\n",
    "        # domain label\n",
    "        domain_label_src = torch.ones(len(src_x)).to(self.device)\n",
    "        domain_label_trg = torch.zeros(len(trg_x)).to(self.device)\n",
    "\n",
    "        # src features\n",
    "        src_feat_tf = self.feature_extractor(\n",
    "            modality_src_x_t, modality_src_x_f)\n",
    "        src_pred = self.classifier_tf(src_feat_tf)\n",
    "\n",
    "        # trg features\n",
    "        trg_feat_tf = self.feature_extractor(\n",
    "            modality_trg_x_t, modality_trg_x_f)\n",
    "\n",
    "        # Task classification  Loss\n",
    "        src_cls_loss = self.cross_entropy(src_pred.squeeze(), src_y)\n",
    "\n",
    "        # Domain classification loss\n",
    "        # source\n",
    "        src_feat_reversed = ReverseLayerF.apply(src_feat_tf, alpha)\n",
    "        src_domain_pred = self.domain_discriminator_tf(src_feat_reversed)\n",
    "        src_domain_loss = self.cross_entropy(\n",
    "            src_domain_pred, domain_label_src.long())\n",
    "\n",
    "        # target\n",
    "        trg_feat_reversed = ReverseLayerF.apply(trg_feat_tf, alpha)\n",
    "        trg_domain_pred = self.domain_discriminator_tf(trg_feat_reversed)\n",
    "        trg_domain_loss = self.cross_entropy(\n",
    "            trg_domain_pred, domain_label_trg.long())\n",
    "\n",
    "        # Total domain loss\n",
    "        domain_loss = src_domain_loss + trg_domain_loss\n",
    "\n",
    "        loss = self.hparams[\"src_cls_loss_wt\"] * src_cls_loss + \\\n",
    "            (1-self.hparams[\"src_cls_loss_wt\"]) * domain_loss\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer_disc.step()\n",
    "\n",
    "        return {'Total_loss': loss.item(), 'Domain_loss': domain_loss.item(), 'Src_cls_loss': src_cls_loss.item()}\n",
    "\n",
    "    def test_batch(self, trg_x):\n",
    "        modality_trg_x_t = torch.split(\n",
    "            trg_x, split_size_or_sections=self.channel_num, dim=1)\n",
    "        modality_trg_x_f = [self.get_f_x(trg_x_t)\n",
    "                            for trg_x_t in modality_trg_x_t]\n",
    "\n",
    "        trg_feat_tf = self.feature_extractor(\n",
    "            modality_trg_x_t, modality_trg_x_f)\n",
    "\n",
    "        trg_pred = self.classifier_tf(trg_feat_tf)\n",
    "        return trg_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ad6ee",
   "metadata": {
    "papermill": {
     "duration": 0.00375,
     "end_time": "2023-07-01T03:46:43.946279",
     "exception": false,
     "start_time": "2023-07-01T03:46:43.942529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Data & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0099f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T03:46:43.955536Z",
     "iopub.status.busy": "2023-07-01T03:46:43.955034Z",
     "iopub.status.idle": "2023-07-01T03:46:45.421411Z",
     "shell.execute_reply": "2023-07-01T03:46:45.420344Z"
    },
    "papermill": {
     "duration": 1.474221,
     "end_time": "2023-07-01T03:46:45.424144",
     "exception": false,
     "start_time": "2023-07-01T03:46:43.949923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def split_valid(dataset):\n",
    "    data = dataset[\"samples\"]\n",
    "    label = dataset[\"labels\"]\n",
    "    train_data, valid_data, train_labels, valid_labels = train_test_split(\n",
    "        data, label, test_size=0.25, shuffle=True, random_state=42)\n",
    "    train_dataset = {\n",
    "        \"samples\": train_data,\n",
    "        \"labels\": train_labels,\n",
    "    }\n",
    "    valid_dataset = {\n",
    "        \"samples\": valid_data,\n",
    "        \"labels\": valid_labels,\n",
    "    }\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "class Load_Dataset(Dataset):\n",
    "    def __init__(self, dataset, normalize):\n",
    "        super(Load_Dataset, self).__init__()\n",
    "\n",
    "        data = dataset[\"samples\"]\n",
    "        label = dataset[\"labels\"]\n",
    "\n",
    "        if len(data.shape) < 3:\n",
    "            data = data.unsqueeze(2)\n",
    "\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data)\n",
    "            label = torch.from_numpy(label).long()\n",
    "\n",
    "        # make sure the Channels in second dim\n",
    "        if data.shape.index(min(data.shape[1], data.shape[2])) != 1:\n",
    "            # 数最小（channel）的dim不是1，要将channel转换到dim1来\n",
    "            data = data.permute(0, 2, 1)  # (N, 128, 6)=>(N, 6, 128)\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "        self.num_channels = data.shape[1]\n",
    "\n",
    "        if normalize:\n",
    "            # Assume datashape: num_samples, num_channels, seq_length\n",
    "            data_mean = torch.FloatTensor(self.num_channels).fill_(\n",
    "                0).tolist()  # assume min= number of channels\n",
    "            data_std = torch.FloatTensor(self.num_channels).fill_(\n",
    "                1).tolist()  # assume min= number of channels\n",
    "            data_transform = transforms.Normalize(mean=data_mean, std=data_std)\n",
    "            self.transform = data_transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "        self.len = data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            output = self.transform(\n",
    "                self.data[index].view(self.num_channels, -1, 1))\n",
    "            self.data[index] = output.view(self.data[index].shape)\n",
    "\n",
    "        return self.data[index].float(), self.label[index].long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def data_generator(data_path, domain_id, dataset_configs, hparams):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            data_path      : 数据文件夹位置\n",
    "            domain_id      : 用户的id\n",
    "            dataset_configs: 数据集配置\n",
    "            hparams        : 超参数配置\n",
    "    \"\"\"\n",
    "    # loading path\n",
    "    train_dataset = torch.load(os.path.join(\n",
    "        data_path, \"train_\" + str(domain_id) + \".pt\"))\n",
    "    test_dataset = torch.load(os.path.join(\n",
    "        data_path, \"test_\" + str(domain_id) + \".pt\"))\n",
    "    train_dataset, valid_dataset = split_valid(train_dataset)\n",
    "\n",
    "    # Loading datasets\n",
    "    train_dataset = Load_Dataset(train_dataset, dataset_configs.normalize)\n",
    "    valid_dataset = Load_Dataset(valid_dataset, dataset_configs.normalize)\n",
    "    test_dataset = Load_Dataset(test_dataset, dataset_configs.normalize)\n",
    "\n",
    "    # Dataloaders\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, drop_last=True, num_workers=2)\n",
    "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, drop_last=True, num_workers=2)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
    "                             shuffle=False, drop_last=dataset_configs.drop_last, num_workers=2)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "def fix_randomness(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  一个epoch内，记录不同batch的测试指标，并返回平均值\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        \"\"\"n是batch增加的个数\"\"\"\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def _calc_metrics(pred_labels, true_labels, target_names):\n",
    "    \"\"\"\n",
    "        在log_dir下保存.csv，记录衡量指标，返回百分制的acc和maf1\n",
    "    \"\"\"\n",
    "    # print(pred_labels.shape)\n",
    "    pred_labels = np.array(pred_labels).astype(int)\n",
    "    true_labels = np.array(true_labels).astype(int)\n",
    "\n",
    "    # precisiion, recall, f1, support\n",
    "    r = classification_report(\n",
    "        true_labels, pred_labels, labels=range(len(target_names)), target_names=target_names, digits=6, output_dict=True)\n",
    "    df = pd.DataFrame(r)\n",
    "\n",
    "    # acc\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    df[\"accuracy\"] = accuracy  # 一列都是重复的值，没有办法\n",
    "\n",
    "    # 转换为百分制\n",
    "    df = df * 100\n",
    "\n",
    "    # 保存结果\n",
    "#     file_name = \"classification_report.csv\"\n",
    "#     report_Save_path = os.path.join(log_dir, file_name)\n",
    "#     df.to_csv(report_Save_path)\n",
    "#     df.to_excel(report_Save_path+\".xlsx\")\n",
    "\n",
    "    return accuracy * 100, r[\"macro avg\"][\"f1-score\"] * 100\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, verbose=True, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.pb = False\n",
    "\n",
    "    def __call__(self, val_loss, model=None, path=None):\n",
    "        self.pb = False\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            print(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).\\n')\n",
    "            self.val_loss_min = val_loss\n",
    "            self.pb = True\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            \n",
    "            self.counter += 1\n",
    "            print(\n",
    "                f'EarlyStopping counter: {self.counter} out of {self.patience}\\n')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            self.pb=True\n",
    "\n",
    "    def refresh(self):\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.pb = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf2025",
   "metadata": {
    "papermill": {
     "duration": 0.004887,
     "end_time": "2023-07-01T03:46:45.435205",
     "exception": false,
     "start_time": "2023-07-01T03:46:45.430318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6bb6de4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T03:46:45.444638Z",
     "iopub.status.busy": "2023-07-01T03:46:45.444305Z",
     "iopub.status.idle": "2023-07-01T03:46:46.139830Z",
     "shell.execute_reply": "2023-07-01T03:46:46.138882Z"
    },
    "papermill": {
     "duration": 0.703031,
     "end_time": "2023-07-01T03:46:46.142197",
     "exception": false,
     "start_time": "2023-07-01T03:46:45.439166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "\n",
    "class trainer():\n",
    "    def __init__(self, args) -> None:\n",
    "        self.method_cls = args.method_cls\n",
    "        self.data_path = args.data_path\n",
    "        self.ds_config = args.ds_configs\n",
    "        self.hparams = args.hparams\n",
    "        self.method_name = args.method_name\n",
    "        self.dataset_name = args.dataset_name\n",
    "        \n",
    "        self.es = EarlyStopping()\n",
    "        self.num_runs = args.num_runs  # 每个任务重复次数（不同seed）\n",
    "        self.device = torch.device(args.device)\n",
    "        # 记录结果\n",
    "\n",
    "    def get_dataloader(self, uid):\n",
    "        \"\"\"\n",
    "            trian_dl, valid_dl, test_dl\n",
    "        \"\"\"\n",
    "        return data_generator(self.data_path, uid, self.ds_config, self.default_hparams)\n",
    "\n",
    "    def evaluate(self, valid=False):\n",
    "        \"\"\"\n",
    "            验证模型性能\n",
    "            \n",
    "            return:\n",
    "                metric: {'accuracy': str, 'maf1_score': str}\n",
    "                loss: str\n",
    "        \"\"\"\n",
    "        method = self.method.to(self.device).eval()\n",
    "\n",
    "        total_loss_ = []\n",
    "\n",
    "        self.trg_pred_labels = []\n",
    "        self.trg_true_labels = []\n",
    "\n",
    "        if valid:\n",
    "            dataloader = self.trg_valid_dl\n",
    "        else:\n",
    "            dataloader = self.trg_test_dl\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, labels in dataloader:\n",
    "                data = data.float().to(self.device)\n",
    "                labels = labels.view((-1)).long().to(self.device)\n",
    "\n",
    "                # forward pass\n",
    "                predictions = method.test_batch(trg_x=data)\n",
    "\n",
    "                # compute loss\n",
    "                loss = F.cross_entropy(predictions, labels)\n",
    "                total_loss_.append(loss.item())\n",
    "                # get the index of the max log-probability\n",
    "                pred = predictions.detach().argmax(dim=1)\n",
    "\n",
    "                self.trg_pred_labels.append(pred.cpu())\n",
    "                self.trg_true_labels.append(labels.cpu())\n",
    "\n",
    "        self.trg_loss = torch.tensor(total_loss_).mean()  # average loss\n",
    "\n",
    "        # n*[BN,]->[n*BN]\n",
    "        self.trg_pred_labels = torch.concat(self.trg_pred_labels, dim=0)\n",
    "        self.trg_true_labels = torch.concat(self.trg_true_labels, dim=0)\n",
    "\n",
    "        # 计算结果\n",
    "        self.acc, self.maf1 = _calc_metrics(self.trg_pred_labels, self.trg_true_labels,\n",
    "                                            self.ds_config.class_names)\n",
    "\n",
    "        self.run_metrics = {'accuracy': self.acc, 'maf1_score': self.maf1}\n",
    "\n",
    "        return self.run_metrics, self.trg_loss\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"\n",
    "            将运行结果汇总生成csv，保存在实验目录下（scenario的上一层）\n",
    "                            acc,    maf1\n",
    "            scenario_run\n",
    "        \"\"\"\n",
    "        self.all_scenario_results = {\n",
    "            \"acc\": {},\n",
    "            \"maf1\": {}\n",
    "        }\n",
    "        for run_name in self.metrics.keys():\n",
    "            self.all_scenario_results[\"acc\"][run_name] = self.metrics[run_name]['accuracy']\n",
    "            self.all_scenario_results[\"maf1\"][run_name] = self.metrics[run_name]['maf1_score']\n",
    "\n",
    "        # 所有scenario_run结果，保存在Exp_logs/Exp_name/exp_id下\n",
    "        self.rs_df = pd.DataFrame(self.all_scenario_results)\n",
    "        file_save_pth = os.path.join(\n",
    "            \"/kaggle/working/\", f\"{self.dataset_name}_{self.method_name}_all_scenario_run_results.csv\")\n",
    "        self.rs_df.to_csv(file_save_pth)\n",
    "        return\n",
    "\n",
    "    def train(self):\n",
    "        scenarios = self.ds_config.scenarios\n",
    "        self.metrics = {}\n",
    "        for i in scenarios:\n",
    "            self.tmp_scenario = f\"{i[0]}_to_{i[1]}\"\n",
    "            src_id = i[0]\n",
    "            trg_id = i[1]\n",
    "\n",
    "\n",
    "            self.run = wandb.init()\n",
    "            cfg = wandb.config  # 不能简写，要先单拿出来config，之后再操作\n",
    "            \n",
    "            # 设置超参数\n",
    "            self.default_hparams = {\n",
    "                **self.hparams.train_params,\n",
    "            }\n",
    "            self.default_hparams['learning_rate'] = cfg.learning_rate\n",
    "            self.default_hparams['src_cls_loss_wt'] = cfg.src_cls_loss_wt\n",
    "            print(self.default_hparams)\n",
    "            \n",
    "            # 获取dataloader\n",
    "            print(f'-----获取dataloader-----')\n",
    "            self.src_train_dl, self.src_valid_dl, self.src_test_dl = self.get_dataloader(\n",
    "                src_id)\n",
    "            self.trg_train_dl, self.trg_valid_dl, self.trg_test_dl = self.get_dataloader(\n",
    "                trg_id)\n",
    "\n",
    "            log_flag = True\n",
    "\n",
    "            seed = 123\n",
    "\n",
    "            # 固定随机种子\n",
    "            fix_randomness(seed)\n",
    "            \n",
    "            \n",
    "            # 类实例化\n",
    "            method = self.method_cls(\n",
    "                configs=self.ds_config, hparams=self.default_hparams, device=self.device).to(self.device)\n",
    "\n",
    "            # AvgMeters\n",
    "            avg_meters = collections.defaultdict(lambda: AverageMeter())\n",
    "\n",
    "            # 训练\n",
    "            for epoch in range(1, self.default_hparams[\"num_epochs\"]+1):\n",
    "\n",
    "                joint_loaders = enumerate(\n",
    "                    zip(self.src_train_dl, self.trg_train_dl))\n",
    "                len_dataloader = min(\n",
    "                    len(self.src_train_dl), len(self.trg_train_dl))\n",
    "\n",
    "                method.train()\n",
    "                for step, ((src_x, src_y), (trg_x, _)) in tqdm(joint_loaders):\n",
    "                    src_x, src_y, trg_x = src_x.float().to(self.device), src_y.long().to(\n",
    "                        self.device), trg_x.float().to(self.device)\n",
    "\n",
    "                    if self.method_name == \"CoDATS\" or self.method_name == 'CoDATS_tf' or self.method_name == 'ATFA':\n",
    "                        loss_dict = method.update(\n",
    "                            src_x=src_x, src_y=src_y, trg_x=trg_x, step=step, epoch=epoch, len_dataloader=len_dataloader)\n",
    "                    else:\n",
    "                        loss_dict = method.update(src_x, src_y, trg_x)\n",
    "\n",
    "                    for key, val in loss_dict.items():\n",
    "                        # batch loss，n=1\n",
    "                        avg_meters[f\"Loss/{key}\"].update(val, n=1)\n",
    "\n",
    "                # 每个epoch输出结果\n",
    "                print(\n",
    "                    f'------------ train epoch: {epoch} ------------------')\n",
    "                log_dict = {}\n",
    "                for key, val in avg_meters.items():\n",
    "                    if \"Loss/\" in key:\n",
    "                        print(f'{key}\\t: {val.avg:2.4f}')\n",
    "                        log_dict[key] = val.avg\n",
    "                        if log_flag:\n",
    "                            self.run.log(\n",
    "                                {f'train/{key}': val.avg}, step=epoch)\n",
    "                print(f'---------------------------------------------------')\n",
    "\n",
    "                # Valid\n",
    "                if epoch % self.default_hparams[\"valid_interval\"] == 0:\n",
    "                    print(\n",
    "                        f'------------- valid epoch: {epoch} ------------------')\n",
    "                    self.method = method\n",
    "                    _, valid_loss = self.evaluate(\n",
    "                        valid=True)  # run_metrics\n",
    "\n",
    "                    self.es(val_loss=valid_loss)\n",
    "                    \n",
    "                    # 这里不保存valid_best 模型了，直接在test上进行测试即可\n",
    "                    if self.es.pb:\n",
    "                        self.method = method\n",
    "                        metric, _ = self.evaluate()\n",
    "                        # self.metrics[f\"{self.tmp_scenario}_seed_{seed}\"] = metric  # 不是最好的结果\n",
    "                        self.run.log(\n",
    "                            {\"test/accuracy\": metric[\"accuracy\"]}, step=epoch)\n",
    "\n",
    "                    if self.es.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "\n",
    "            self.es.refresh()\n",
    "            # Train Done, Test\n",
    "            \n",
    "            \n",
    "        # scenario Done\n",
    "        # self.save_results()  # sweep不需要这个\n",
    "    # end train\n",
    "# end class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3ea54",
   "metadata": {
    "papermill": {
     "duration": 0.003753,
     "end_time": "2023-07-01T03:46:46.150195",
     "exception": false,
     "start_time": "2023-07-01T03:46:46.146442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Train script steup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67acf1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T03:46:46.159502Z",
     "iopub.status.busy": "2023-07-01T03:46:46.159219Z",
     "iopub.status.idle": "2023-07-01T03:46:46.175547Z",
     "shell.execute_reply": "2023-07-01T03:46:46.174753Z"
    },
    "papermill": {
     "duration": 0.023595,
     "end_time": "2023-07-01T03:46:46.177514",
     "exception": false,
     "start_time": "2023-07-01T03:46:46.153919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class CNN_configs(object):\n",
    "    def __init__(self) -> None:\n",
    "        super(CNN_configs, self).__init__()\n",
    "        self.input_channels = 3  # 模态通道总数\n",
    "        self.kernel_size = 5  # 第一层kernel_size\n",
    "        self.stride = 1  # 第一层stride\n",
    "        self.dropout = 0.5  # 第一层dropout\n",
    "\n",
    "        self.mid_channels = 64  # 中间通道数\n",
    "        self.final_out_channels = 128  # 最后一层输出通道数\n",
    "        self.features_len = 1  # avgpool 平均结果长度\n",
    "\n",
    "\n",
    "class Classifier_configs(object):\n",
    "    def __init__(self) -> None:\n",
    "        super(Classifier_configs, self).__init__()\n",
    "        self.features_len = 1  # CNN avgpool 平均结果长度\n",
    "        self.hidden_dim = 500\n",
    "        self.final_out_channels = 128  # CNN 最后一层输出通道数\n",
    "\n",
    "\n",
    "class Discriminator_configs(object):\n",
    "    def __init__(self) -> None:\n",
    "        super(Discriminator_configs, self).__init__()\n",
    "        self.final_out_channels = 128  # CNN最后一层输出通道数\n",
    "        self.features_len = 1  # CNN avgpool 平均结果长度\n",
    "        self.disc_hid_dim = 64\n",
    "\n",
    "\n",
    "class fusion_configs(object):\n",
    "    def __init__(self) -> None:\n",
    "        super(fusion_configs, self).__init__()\n",
    "        self.final_out_channels = 128  # cnn_rs.shape[-1]\n",
    "        self.hidden_size = 500\n",
    "\n",
    "\n",
    "class PAMAP2_configs(object):  # HHAR dataset, SAMSUNG device.\n",
    "    def __init__(self):\n",
    "        super(PAMAP2_configs, self).__init__()\n",
    "        self.sequence_len = 200\n",
    "        self.num_users = 9  # 1~9\n",
    "\n",
    "        self.scenarios = [(\"1\", \"7\")]\n",
    "        self.num_classes = 18  # 0~17\n",
    "        self.class_names = ['lying' 'sitting', 'standing', 'walking', 'running',\n",
    "                            'cycling', 'Nordic walking', 'watching TV', 'computer work', 'car driving',\n",
    "                            'ascending stairs', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry',\n",
    "                            'house cleaning', 'playing soccer', 'rope jumping']\n",
    "\n",
    "        self.shuffle = True\n",
    "        self.drop_last = True\n",
    "        self.normalize = True\n",
    "        self.modality_nums = 3 * 3  # 传感器模态数\n",
    "        self.channel_nums = [3, 3, 3] + [3, 3, 3] + [3, 3, 3]  # 各个模态通道数\n",
    "\n",
    "        self.model_configs = {\n",
    "            'CNN': CNN_configs(),\n",
    "            'fusion': fusion_configs(),\n",
    "            'Classifier': Classifier_configs(),\n",
    "            'Discriminator': Discriminator_configs(),\n",
    "        }\n",
    "\n",
    "\n",
    "# -------------------- 训练配置 ---------------------------\n",
    "class PAMAP2_hparams():\n",
    "    def __init__(self):\n",
    "        super(PAMAP2_hparams, self).__init__()\n",
    "        self.train_params = {\n",
    "            'num_epochs': 100,  # 总训练轮数\n",
    "            'batch_size': 128,  # 每个域的batch数\n",
    "            'weight_decay': 1e-4,\n",
    "            'valid_interval': 2,\n",
    "        }\n",
    "        # self.alg_hparams = {\n",
    "        #     'ATFA':       {'learning_rate': 0.0005,   'src_cls_loss_wt': 7.737,  'domain_loss_wt': 3.369},\n",
    "        # }\n",
    "\n",
    "\n",
    "class args():\n",
    "    def __init__(self) -> None:\n",
    "        self.method_cls = ATFA\n",
    "        self.hparams = PAMAP2_hparams()\n",
    "        self.ds_configs = PAMAP2_configs()\n",
    "        self.data_path = \"/kaggle/input/pamap2accgyromag/PAMAP2_data\"\n",
    "        self.num_runs = 1\n",
    "        self.device = \"cuda:0\"\n",
    "        self.method_name = \"ATFA\"\n",
    "        self.dataset_name = \"PAMAP2\"\n",
    "\n",
    "\n",
    "args = args()\n",
    "trainer = trainer(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48a501",
   "metadata": {
    "papermill": {
     "duration": 0.003651,
     "end_time": "2023-07-01T03:46:46.184967",
     "exception": false,
     "start_time": "2023-07-01T03:46:46.181316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Sweep setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73250c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T03:46:46.194294Z",
     "iopub.status.busy": "2023-07-01T03:46:46.193668Z",
     "iopub.status.idle": "2023-07-01T05:52:25.686673Z",
     "shell.execute_reply": "2023-07-01T05:52:25.685119Z"
    },
    "papermill": {
     "duration": 7539.500377,
     "end_time": "2023-07-01T05:52:25.689211",
     "exception": false,
     "start_time": "2023-07-01T03:46:46.188834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import numpy as np\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# I have saved my API token with \"wandb_api\" as Label.\n",
    "# If you use some other Label make sure to change the same below.\n",
    "wandb_api = user_secrets.get_secret(\n",
    "    \"wandb_api\")  # Add-ons => Secrets 选中 keys 才能用\n",
    "\n",
    "metric = {\n",
    "    \"name\": 'test/accuracy',  # wandb.log.key_name\n",
    "    'goal': 'maximize',\n",
    "}\n",
    "weight_list = np.arange(start=60, stop=80)*0.01\n",
    "parameters_dict = {\n",
    "    'learning_rate': {'values':[0.0005, 0.001]},\n",
    "    'src_cls_loss_wt': {'values':weight_list.tolist()},\n",
    "    # 'domain_loss_wt' = 1- src_cls_loss_wt,\n",
    "}\n",
    "\n",
    "sweep_cfg = {\n",
    "    'method': 'grid',\n",
    "    'metric': metric,\n",
    "    'parameters': parameters_dict,\n",
    "}\n",
    "\n",
    "wandb.login(key=wandb_api)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_cfg, project=\"atfa-pamap2-sweep\")\n",
    "\n",
    "wandb.agent(sweep_id, trainer.train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7561.884368,
   "end_time": "2023-07-01T05:52:31.612324",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-01T03:46:29.727956",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
