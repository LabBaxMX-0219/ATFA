{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90a1935",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-15T08:27:14.513010Z",
     "iopub.status.busy": "2023-03-15T08:27:14.512440Z",
     "iopub.status.idle": "2023-03-15T08:27:14.530920Z",
     "shell.execute_reply": "2023-03-15T08:27:14.529564Z"
    },
    "papermill": {
     "duration": 0.027715,
     "end_time": "2023-03-15T08:27:14.533846",
     "exception": false,
     "start_time": "2023-03-15T08:27:14.506131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/opportunity-adl/S2-ADL2.dat\n",
      "/kaggle/input/opportunity-adl/S3-ADL1.dat\n",
      "/kaggle/input/opportunity-adl/S3-ADL2.dat\n",
      "/kaggle/input/opportunity-adl/S3-ADL4.dat\n",
      "/kaggle/input/opportunity-adl/S2-ADL4.dat\n",
      "/kaggle/input/opportunity-adl/S1-ADL3.dat\n",
      "/kaggle/input/opportunity-adl/S3-ADL3.dat\n",
      "/kaggle/input/opportunity-adl/S4-ADL2.dat\n",
      "/kaggle/input/opportunity-adl/label_legend.txt\n",
      "/kaggle/input/opportunity-adl/S1-ADL5.dat\n",
      "/kaggle/input/opportunity-adl/S1-ADL4.dat\n",
      "/kaggle/input/opportunity-adl/S3-ADL5.dat\n",
      "/kaggle/input/opportunity-adl/S2-ADL3.dat\n",
      "/kaggle/input/opportunity-adl/S4-ADL3.dat\n",
      "/kaggle/input/opportunity-adl/S4-ADL1.dat\n",
      "/kaggle/input/opportunity-adl/S1-ADL2.dat\n",
      "/kaggle/input/opportunity-adl/S1-ADL1.dat\n",
      "/kaggle/input/opportunity-adl/S2-ADL1.dat\n",
      "/kaggle/input/opportunity-adl/column_names.txt\n",
      "/kaggle/input/opportunity-adl/S4-ADL4.dat\n",
      "/kaggle/input/opportunity-adl/S4-ADL5.dat\n",
      "/kaggle/input/opportunity-adl/S2-ADL5.dat\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5e6f2",
   "metadata": {
    "papermill": {
     "duration": 0.002815,
     "end_time": "2023-03-15T08:27:14.540053",
     "exception": false,
     "start_time": "2023-03-15T08:27:14.537238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "用户：1~4《\n",
    "采样频率：100hz\n",
    "长度：300（3s）<br>\n",
    "重叠度：30<br>\n",
    "文件：ADL<br>\n",
    "活动：HL_5（Relaxing101, Coffee time102, Early morning103, Cleanup104, Sandwich time105）<br>\n",
    "传感器：acc、gyro、magnet<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190c53b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T08:27:14.548389Z",
     "iopub.status.busy": "2023-03-15T08:27:14.547502Z",
     "iopub.status.idle": "2023-03-15T08:27:14.559858Z",
     "shell.execute_reply": "2023-03-15T08:27:14.558897Z"
    },
    "papermill": {
     "duration": 0.019429,
     "end_time": "2023-03-15T08:27:14.562457",
     "exception": false,
     "start_time": "2023-03-15T08:27:14.543028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ['/kaggle/input/opportunity-adl/S1-ADL1.dat',\n",
       "  '/kaggle/input/opportunity-adl/S1-ADL2.dat',\n",
       "  '/kaggle/input/opportunity-adl/S1-ADL3.dat',\n",
       "  '/kaggle/input/opportunity-adl/S1-ADL4.dat',\n",
       "  '/kaggle/input/opportunity-adl/S1-ADL5.dat'],\n",
       " '2': ['/kaggle/input/opportunity-adl/S2-ADL1.dat',\n",
       "  '/kaggle/input/opportunity-adl/S2-ADL2.dat',\n",
       "  '/kaggle/input/opportunity-adl/S2-ADL3.dat',\n",
       "  '/kaggle/input/opportunity-adl/S2-ADL4.dat',\n",
       "  '/kaggle/input/opportunity-adl/S2-ADL5.dat'],\n",
       " '3': ['/kaggle/input/opportunity-adl/S3-ADL1.dat',\n",
       "  '/kaggle/input/opportunity-adl/S3-ADL2.dat',\n",
       "  '/kaggle/input/opportunity-adl/S3-ADL3.dat',\n",
       "  '/kaggle/input/opportunity-adl/S3-ADL4.dat',\n",
       "  '/kaggle/input/opportunity-adl/S3-ADL5.dat'],\n",
       " '4': ['/kaggle/input/opportunity-adl/S4-ADL1.dat',\n",
       "  '/kaggle/input/opportunity-adl/S4-ADL2.dat',\n",
       "  '/kaggle/input/opportunity-adl/S4-ADL3.dat',\n",
       "  '/kaggle/input/opportunity-adl/S4-ADL4.dat',\n",
       "  '/kaggle/input/opportunity-adl/S4-ADL5.dat']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "src_dir = \"/kaggle/input/opportunity-adl\"\n",
    "user_num = 4\n",
    "adl_file_num = 5\n",
    "files_dir = {}\n",
    "for user_idx in range(1, user_num+1):\n",
    "    user_data_path = []\n",
    "    for file_idx in range(1, adl_file_num+1):\n",
    "        file_name = f\"S{user_idx}-ADL{file_idx}.dat\"\n",
    "        file_path = os.path.join(src_dir, file_name)\n",
    "        user_data_path.append(file_path)\n",
    "    files_dir[str(user_idx)] = user_data_path\n",
    "\n",
    "files_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c318c477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T08:27:14.571893Z",
     "iopub.status.busy": "2023-03-15T08:27:14.571056Z",
     "iopub.status.idle": "2023-03-15T08:27:14.591254Z",
     "shell.execute_reply": "2023-03-15T08:27:14.589923Z"
    },
    "papermill": {
     "duration": 0.028548,
     "end_time": "2023-03-15T08:27:14.594432",
     "exception": false,
     "start_time": "2023-03-15T08:27:14.565884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_cleaned_user_data(file_pth, user_idx):\n",
    "    \"\"\"去除不用传感器模态，去除活动转换数据，插值补全NaN数据\"\"\"\n",
    "    invalid_feature = np.arange( 46, 50 )  # BACK Quaternion\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(34, 37)] )  # RH_acc\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(59, 63)] )  # RUA Quaternion\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(72, 76)] )  # RLA\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(85, 89)] )  # LUA\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(99, 102)] )  # LLA\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(117, 118)] )  # L-SHOE Compass\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(133, 134)] )  # R-SHOE Compass\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(134, 244)] )  # environment sensor\n",
    "    invalid_feature = np.concatenate( [invalid_feature, np.arange(245, 250)] )  # LL, ML level label\n",
    "    drop_columns = invalid_feature\n",
    "#     print(drop_columns, len(drop_columns))\n",
    "    raw_data = np.loadtxt(file_pth)\n",
    "#     print(raw_data.shape)\n",
    "    used_data = np.delete(raw_data, drop_columns, axis=1)\n",
    "    print(used_data.shape)    \n",
    "    \n",
    "    used_columns = [\"MILLISEC\",\n",
    "                    \"acc_RKN_upper_accX\",\"acc_RKN_upper_accY\",\"acc_RKN_upper_accZ\",\n",
    "                    \"acc_HIP_accX\",\"acc_HIP_accY\",\"acc_HIP_accZ\",\n",
    "                    \"acc_LUA_upper_accX\",\"acc_LUA_upper_accY\",\"acc_LUA_upper_accZ\",\n",
    "                    \"acc_RUA_lower_accX\",\"acc_RUA_lower_accY\",\"acc_RUA_lower_accZ\",\n",
    "                    \"acc_LH_accX\",\"acc_LH_accY\",\"acc_LH_accZ\",\n",
    "                    \"acc_BACK_accX\",\"acc_BACK_accY\",\"acc_BACK_accZ\",\n",
    "                    \"acc_RKN_lower_accX\",\"acc_RKN_lower_accY\",\"acc_RKN_lower_accZ\",\n",
    "                    \"acc_RWR_accX\",\"acc_RWR_accY\",\"acc_RWR_accZ\",\n",
    "                    \"acc_RUA_upper_accX\",\"acc_RUA_upper_accY\",\"acc_RUA_upper_accZ\",\n",
    "                    \"acc_LUA_lower_accX\",\"acc_LUA_lower_accY\",\"acc_LUA_lower_accZ\",\n",
    "                    \"acc_LWR_accX\",\"acc_LWR_accY\",\"acc_LWR_accZ\",\n",
    "#                     \"acc_RH_accX\",\"acc_RH_accY\",\"acc_RH_accZ\",\n",
    "                    \"imu_BACK_accX\",\"imu_BACK_accY\",\"imu_BACK_accZ\",\n",
    "                    \"imu_BACK_gyroX\",\"imu_BACK_gyroY\",\"imu_BACK_gyroZ\",\n",
    "                    \"imu_BACK_magneticX\",\"imu_BACK_magneticY\",\"imu_BACK_magneticZ\",\n",
    "                    \"imu_RUA_accX\",\"imu_RUA_accY\",\"imu_RUA_accZ\",\n",
    "                    \"imu_RUA_gyroX\",\"imu_RUA_gyroY\",\"imu_RUA_gyroZ\",\n",
    "                    \"imu_RUA_magneticX\",\"imu_RUA_magneticY\",\"imu_RUA_magneticZ\",\n",
    "                    \"imu_RLA_accX\",\"imu_RLA_accY\",\"imu_RLA_accZ\",\n",
    "                    \"imu_RLA_gyroX\",\"imu_RLA_gyroY\",\"imu_RLA_gyroZ\",\n",
    "                    \"imu_RLA_magneticX\",\"imu_RLA_magneticY\",\"imu_RLA_magneticZ\",\n",
    "                    \"imu_LUA_accX\",\"imu_LUA_accY\",\"imu_LUA_accZ\",\n",
    "                    \"imu_LUA_gyroX\",\"imu_LUA_gyroY\",\"imu_LUA_gyroZ\",\n",
    "                    \"imu_LUA_magneticX\",\"imu_LUA_magneticY\",\"imu_LUA_magneticZ\",\n",
    "                    \"imu_LLA_accX\",\"imu_LLA_accY\",\"imu_LLA_accZ\",\n",
    "                    \"imu_LLA_gyroX\",\"imu_LLA_gyroY\",\"imu_LLA_gyroZ\",\n",
    "                    \"imu_LLA_magneticX\",\"imu_LLA_magneticY\",\"imu_LLA_magneticZ\",\n",
    "                    \"imu_L-SHOE_EuX\",\"imu_L-SHOE_EuY\",\"imu_L-SHOE_EuZ\",\n",
    "                    \"imu_L-SHOE_Nav_Ax\",\"imu_L-SHOE_Nav_Ay\",\"imu_L-SHOE_Nav_Az\",\n",
    "                    \"imu_L-SHOE_Body_Ax\",\"imu_L-SHOE_Body_Ay\",\"imu_L-SHOE_Body_Az\",\n",
    "                    \"imu_L-SHOE_AngVelBodyFrameX\",\"imu_L-SHOE_AngVelBodyFrameY\",\"imu_L-SHOE_AngVelBodyFrameZ\",\n",
    "                    \"imu_L-SHOE_AngVelNavFrameX\",\"imu_L-SHOE_AngVelNavFrameY\",\"imu_L-SHOE_AngVelNavFrameZ\",\n",
    "                    \"imu_R-SHOE_EuX\",\"imu_R-SHOE_EuY\",\"imu_R-SHOE_EuZ\",\n",
    "                    \"imu_R-SHOE_Nav_Ax\",\"imu_R-SHOE_Nav_Ay\",\"imu_R-SHOE_Nav_Az\",\n",
    "                    \"imu_R-SHOE_Body_Ax\",\"imu_R-SHOE_Body_Ay\",\"imu_R-SHOE_Body_Az\",\n",
    "                    \"imu_R-SHOE_AngVelBodyFrameX\",\"imu_R-SHOE_AngVelBodyFrameY\",\"imu_R-SHOE_AngVelBodyFrameZ\",\n",
    "                    \"imu_R-SHOE_AngVelNavFrameX\",\"imu_R-SHOE_AngVelNavFrameY\",\"imu_R-SHOE_AngVelNavFrameZ\",\n",
    "                    \"Locomotion\",\n",
    "                    \"HL_Activity\"]\n",
    "    used_data = pd.DataFrame(used_data, columns=used_columns)\n",
    "#     print(used_data.shape)\n",
    "    used_data = used_data[used_data['HL_Activity'] != 0]  # 活动转换数据标签为0，丢弃\n",
    "#     print(used_data.shape)\n",
    "\n",
    "    used_data['HL_Activity'][used_data['HL_Activity']==101] = 0  # Relaxing\n",
    "    used_data['HL_Activity'][used_data['HL_Activity']==102] = 1  # Coffee time\n",
    "    used_data['HL_Activity'][used_data['HL_Activity']==103] = 2  # Early morning\n",
    "    used_data['HL_Activity'][used_data['HL_Activity']==104] = 3  # Cleanup\n",
    "    used_data['HL_Activity'][used_data['HL_Activity']==105] = 4  # Sandwich time\n",
    "    \n",
    "#     print(used_data.shape)\n",
    "    used_data = used_data.interpolate()\n",
    "#     print(used_data.shape)\n",
    "\n",
    "    # 查看Nan数据所在位置\n",
    "    pos = used_data.isnull().stack()[lambda x:x].index.tolist()\n",
    "#     print(pos)\n",
    "    \n",
    "    used_data = used_data.dropna(axis=0)\n",
    "    print(used_data.shape)\n",
    "    return used_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "993a69b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T08:27:14.603330Z",
     "iopub.status.busy": "2023-03-15T08:27:14.602865Z",
     "iopub.status.idle": "2023-03-15T08:27:15.768983Z",
     "shell.execute_reply": "2023-03-15T08:27:15.767466Z"
    },
    "papermill": {
     "duration": 1.174436,
     "end_time": "2023-03-15T08:27:15.772428",
     "exception": false,
     "start_time": "2023-03-15T08:27:14.597992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def sliding_window(time_series, width, step, order='F'):\n",
    "    w = np.hstack([time_series[i:1 + i - width or None:step] for i in range(0, width)])\n",
    "    result = w.reshape((int(len(w) / width), width), order='F')\n",
    "    if order == 'F':\n",
    "        return result\n",
    "    else:\n",
    "        return np.ascontiguousarray(result)\n",
    "    \n",
    "def calc_normalization(data):\n",
    "    num_instances, num_time_steps, num_features = data.shape\n",
    "    data = np.reshape(data, (num_instances, -1))\n",
    "    scaler.fit(data)\n",
    "#     mean, std = (np.array([np.mean(x) for x in X_train], dtype=np.float32), np.array([np.std(x) for x in X_train], dtype=np.float32))\n",
    "    return scaler\n",
    "    \n",
    "def apply_normalization(data, scaler):\n",
    "#     scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = data.shape\n",
    "    data = np.reshape(data, (num_instances, -1))\n",
    "    norm_data = scaler.transform(data)\n",
    "#     debug_here()\n",
    "#     data = (data - mean) / (std + 1e-5)\n",
    "    norm_data[np.isnan(norm_data)] = 0\n",
    "    norm_data = np.reshape(norm_data, (num_instances, num_time_steps, num_features))\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3da631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T08:27:15.781931Z",
     "iopub.status.busy": "2023-03-15T08:27:15.781503Z",
     "iopub.status.idle": "2023-03-15T08:31:10.254699Z",
     "shell.execute_reply": "2023-03-15T08:31:10.252964Z"
    },
    "papermill": {
     "duration": 234.482443,
     "end_time": "2023-03-15T08:31:10.258566",
     "exception": false,
     "start_time": "2023-03-15T08:27:15.776123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_dir = \"/kaggle/input/opportunity-adl\"\n",
    "user_num = 4\n",
    "adl_file_num = 5\n",
    "\n",
    "seq_length = 300\n",
    "shifting_step = 30\n",
    "channel_num = 3*36\n",
    "used_channels = [\n",
    "    \"acc_RKN_upper_accX\",\"acc_RKN_upper_accY\",\"acc_RKN_upper_accZ\",\n",
    "    \"acc_HIP_accX\",\"acc_HIP_accY\",\"acc_HIP_accZ\",\n",
    "    \"acc_LUA_upper_accX\",\"acc_LUA_upper_accY\",\"acc_LUA_upper_accZ\",\n",
    "    \"acc_RUA_lower_accX\",\"acc_RUA_lower_accY\",\"acc_RUA_lower_accZ\",\n",
    "    \"acc_LH_accX\",\"acc_LH_accY\",\"acc_LH_accZ\",\n",
    "    \"acc_BACK_accX\",\"acc_BACK_accY\",\"acc_BACK_accZ\",\n",
    "    \"acc_RKN_lower_accX\",\"acc_RKN_lower_accY\",\"acc_RKN_lower_accZ\",\n",
    "    \"acc_RWR_accX\",\"acc_RWR_accY\",\"acc_RWR_accZ\",\n",
    "    \"acc_RUA_upper_accX\",\"acc_RUA_upper_accY\",\"acc_RUA_upper_accZ\",\n",
    "    \"acc_LUA_lower_accX\",\"acc_LUA_lower_accY\",\"acc_LUA_lower_accZ\",\n",
    "    \"acc_LWR_accX\",\"acc_LWR_accY\",\"acc_LWR_accZ\",\n",
    "    \"imu_BACK_accX\",\"imu_BACK_accY\",\"imu_BACK_accZ\",\n",
    "    \"imu_BACK_gyroX\",\"imu_BACK_gyroY\",\"imu_BACK_gyroZ\",\n",
    "    \"imu_BACK_magneticX\",\"imu_BACK_magneticY\",\"imu_BACK_magneticZ\",\n",
    "    \"imu_RUA_accX\",\"imu_RUA_accY\",\"imu_RUA_accZ\",\n",
    "    \"imu_RUA_gyroX\",\"imu_RUA_gyroY\",\"imu_RUA_gyroZ\",\n",
    "    \"imu_RUA_magneticX\",\"imu_RUA_magneticY\",\"imu_RUA_magneticZ\",\n",
    "    \"imu_RLA_accX\",\"imu_RLA_accY\",\"imu_RLA_accZ\",\n",
    "    \"imu_RLA_gyroX\",\"imu_RLA_gyroY\",\"imu_RLA_gyroZ\",\n",
    "    \"imu_RLA_magneticX\",\"imu_RLA_magneticY\",\"imu_RLA_magneticZ\",\n",
    "    \"imu_LUA_accX\",\"imu_LUA_accY\",\"imu_LUA_accZ\",\n",
    "    \"imu_LUA_gyroX\",\"imu_LUA_gyroY\",\"imu_LUA_gyroZ\",\n",
    "    \"imu_LUA_magneticX\",\"imu_LUA_magneticY\",\"imu_LUA_magneticZ\",\n",
    "    \"imu_LLA_accX\",\"imu_LLA_accY\",\"imu_LLA_accZ\",\n",
    "    \"imu_LLA_gyroX\",\"imu_LLA_gyroY\",\"imu_LLA_gyroZ\",\n",
    "    \"imu_LLA_magneticX\",\"imu_LLA_magneticY\",\"imu_LLA_magneticZ\",\n",
    "    \"imu_L-SHOE_EuX\",\"imu_L-SHOE_EuY\",\"imu_L-SHOE_EuZ\",\n",
    "    \"imu_L-SHOE_Nav_Ax\",\"imu_L-SHOE_Nav_Ay\",\"imu_L-SHOE_Nav_Az\",\n",
    "    \"imu_L-SHOE_Body_Ax\",\"imu_L-SHOE_Body_Ay\",\"imu_L-SHOE_Body_Az\",\n",
    "    \"imu_L-SHOE_AngVelBodyFrameX\",\"imu_L-SHOE_AngVelBodyFrameY\",\"imu_L-SHOE_AngVelBodyFrameZ\",\n",
    "    \"imu_L-SHOE_AngVelNavFrameX\",\"imu_L-SHOE_AngVelNavFrameY\",\"imu_L-SHOE_AngVelNavFrameZ\",\n",
    "    \"imu_R-SHOE_EuX\",\"imu_R-SHOE_EuY\",\"imu_R-SHOE_EuZ\",\n",
    "    \"imu_R-SHOE_Nav_Ax\",\"imu_R-SHOE_Nav_Ay\",\"imu_R-SHOE_Nav_Az\",\n",
    "    \"imu_R-SHOE_Body_Ax\",\"imu_R-SHOE_Body_Ay\",\"imu_R-SHOE_Body_Az\",\n",
    "    \"imu_R-SHOE_AngVelBodyFrameX\",\"imu_R-SHOE_AngVelBodyFrameY\",\"imu_R-SHOE_AngVelBodyFrameZ\",\n",
    "    \"imu_R-SHOE_AngVelNavFrameX\",\"imu_R-SHOE_AngVelNavFrameY\",\"imu_R-SHOE_AngVelNavFrameZ\",\n",
    "]\n",
    "\n",
    "\n",
    "for user_idx in range(1, user_num+1):\n",
    "    user_data, user_labels = [], []\n",
    "    for file_idx in range(1, adl_file_num+1):\n",
    "        # gen src_data path\n",
    "        file_name = f\"S{user_idx}-ADL{file_idx}.dat\"\n",
    "        file_path = os.path.join(src_dir, file_name)\n",
    "        \n",
    "        # load cleaned data\n",
    "        used_data = get_cleaned_user_data(file_path, user_idx)\n",
    "        \n",
    "        # split data by label\n",
    "        for act_id, act_data in used_data.groupby('HL_Activity'):\n",
    "#             print(act_id)\n",
    "#             print(act_data.shape)\n",
    "            sample_cnt = int((act_data.shape[0]-seq_length)//shifting_step + 1)\n",
    "            if sample_cnt < 2:\n",
    "                print(f\"user {user_index} has only {act_data.shape[0]} samplings, drop\\n\")\n",
    "                continue\n",
    "            data_shape = (sample_cnt, seq_length, channel_num)  # (N, 300, 3*36)\n",
    "            act_sliced_data = np.empty(data_shape)  \n",
    "            channl_idx = 0\n",
    "            for channel_name in used_channels:\n",
    "                channel_data = act_data[channel_name]\n",
    "                act_sliced_data[:,:,channl_idx] = sliding_window(channel_data.values, seq_length, shifting_step, 'T')\n",
    "                channl_idx += 1\n",
    "\n",
    "            # append label data \n",
    "            user_data.append(act_sliced_data)\n",
    "            # gen labels\n",
    "            class_labels = np.empty(act_sliced_data.shape[0])\n",
    "            actual_label = int(act_id)\n",
    "            class_labels.fill(actual_label)\n",
    "            user_labels.append(class_labels.astype(int))\n",
    "            \n",
    "    # data and labels for each users \n",
    "    array_user_data= np.concatenate(user_data, axis=0)\n",
    "    array_user_labels= np.concatenate(user_labels, axis=0)\n",
    "    # print(user_idx, array_user_data.shape, array_user_labels.shape)\n",
    "    \n",
    "    # Stratified train, validation, test split of the data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(array_user_data, array_user_labels,  stratify=array_user_labels,  test_size=0.3,random_state=1)\n",
    "    # print(X_train.shape)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    # Data normalization \n",
    "    # Calculate mean and standard deviation based on train\n",
    "    scaler = calc_normalization(X_train)\n",
    "\n",
    "    # Apply normalization \n",
    "    X_train = apply_normalization(X_train,scaler)\n",
    "    X_test = apply_normalization(X_test,scaler)\n",
    "\n",
    "    print(f\"user: {user_idx}\")\n",
    "    print(f\"train data: {X_train.shape}, train label: {y_train.shape}\")\n",
    "    print(f\"test data: {X_test.shape}, test label: {y_test.shape}\\n\")\n",
    "    \n",
    "    # prepare samples\n",
    "    train_data = {'samples':X_train, 'labels':y_train}\n",
    "    test_data  = {'samples':X_test, 'labels':y_test}\n",
    "\n",
    "    os.makedirs(f'/kaggle/working/OPPORTUNITY_data', exist_ok=True)\n",
    "    torch.save(train_data, f'/kaggle/working/OPPORTUNITY_data/train_{user_idx}.pt')\n",
    "    # torch.save(val_data,  f'HHAR_user_data/val_{user_name}.pt')\n",
    "    torch.save(test_data, f'/kaggle/working/OPPORTUNITY_data/test_{user_idx}.pt')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08027a5f",
   "metadata": {
    "papermill": {
     "duration": 0.005069,
     "end_time": "2023-03-15T08:31:10.269194",
     "exception": false,
     "start_time": "2023-03-15T08:31:10.264125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 248.697776,
   "end_time": "2023-03-15T08:31:11.705499",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-15T08:27:03.007723",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
