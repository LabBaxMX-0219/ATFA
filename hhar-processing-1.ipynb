{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306040a5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-29T16:40:30.476250Z",
     "iopub.status.busy": "2023-01-29T16:40:30.475845Z",
     "iopub.status.idle": "2023-01-29T16:40:30.492827Z",
     "shell.execute_reply": "2023-01-29T16:40:30.492021Z"
    },
    "papermill": {
     "duration": 0.024074,
     "end_time": "2023-01-29T16:40:30.495250",
     "exception": false,
     "start_time": "2023-01-29T16:40:30.471176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hharraw/Phones_accelerometer.csv\n",
      "/kaggle/input/hharraw/Phones_gyroscope.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5605885e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T16:40:30.501644Z",
     "iopub.status.busy": "2023-01-29T16:40:30.501141Z",
     "iopub.status.idle": "2023-01-29T16:40:32.925332Z",
     "shell.execute_reply": "2023-01-29T16:40:32.924389Z"
    },
    "papermill": {
     "duration": 2.429566,
     "end_time": "2023-01-29T16:40:32.927430",
     "exception": false,
     "start_time": "2023-01-29T16:40:30.497864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7686542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T16:40:32.933555Z",
     "iopub.status.busy": "2023-01-29T16:40:32.933126Z",
     "iopub.status.idle": "2023-01-29T16:41:20.661777Z",
     "shell.execute_reply": "2023-01-29T16:41:20.660510Z"
    },
    "papermill": {
     "duration": 47.734672,
     "end_time": "2023-01-29T16:41:20.664529",
     "exception": false,
     "start_time": "2023-01-29T16:40:32.929857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/hharraw'\n",
    "HHAR_phones_acc = pd.read_csv(f'{data_dir}/Phones_accelerometer.csv')\n",
    "HHAR_phones_gyro = pd.read_csv(f'{data_dir}/Phones_gyroscope.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5a2112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T16:41:20.671006Z",
     "iopub.status.busy": "2023-01-29T16:41:20.670614Z",
     "iopub.status.idle": "2023-01-29T16:41:20.677192Z",
     "shell.execute_reply": "2023-01-29T16:41:20.675908Z"
    },
    "papermill": {
     "duration": 0.012134,
     "end_time": "2023-01-29T16:41:20.679241",
     "exception": false,
     "start_time": "2023-01-29T16:41:20.667107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_Model_data(acc_data, gyro_data):\n",
    "    models = ['nexus4', 's3', 's3mini', 'samsungold']\n",
    "    model_dats = {}\n",
    "    for model in models:\n",
    "        model_dats[model] = []\n",
    "        model_dats[model].append(acc_data[acc_data['Model']==model])\n",
    "        model_dats[model].append(gyro_data[gyro_data['Model']==model])\n",
    "    return model_dats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80b366d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T16:41:20.685553Z",
     "iopub.status.busy": "2023-01-29T16:41:20.685229Z",
     "iopub.status.idle": "2023-01-29T16:41:20.698034Z",
     "shell.execute_reply": "2023-01-29T16:41:20.697027Z"
    },
    "papermill": {
     "duration": 0.018084,
     "end_time": "2023-01-29T16:41:20.699874",
     "exception": false,
     "start_time": "2023-01-29T16:41:20.681790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sensor_pairing(user_acc_data, user_gyro_data, label):\n",
    "    '''columns:\n",
    "        Creation_Time   x   y   z   User,   labels\n",
    "    '''\n",
    "    # 重置索引\n",
    "    user_acc_data = user_acc_data.reset_index(drop=True)\n",
    "    user_gyro_data = user_gyro_data.reset_index(drop=True)\n",
    "\n",
    "    new_pd = pd.DataFrame(columns=['Time', 'acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'labels'])\n",
    "\n",
    "    # 转化为秒级时间戳\n",
    "    timeScale = 100000000.\n",
    "    user_acc_data['Creation_Time'] /= timeScale\n",
    "    user_gyro_data['Creation_Time'] /= timeScale\n",
    "    # print(user_acc_data.head())\n",
    "    # print(user_gyro_data.head())\n",
    "\n",
    "    len_acc = user_acc_data.shape[0]\n",
    "    len_gyro = user_gyro_data.shape[0]\n",
    "    # print(len_acc, len_gyro)\n",
    "    acc_idx = 0\n",
    "    gyro_idx = 0\n",
    "    with tqdm(total=len_acc, position=0, desc=\"Pairing user's label data\") as pbar:\n",
    "        while acc_idx < len_acc and gyro_idx < len_gyro:\n",
    "            acc_curTime = user_acc_data.iloc[acc_idx]['Creation_Time']\n",
    "            # acc_curTime = acc_sample['Creation_Time']\n",
    "            gyro_curTime = user_gyro_data.iloc[gyro_idx]['Creation_Time']\n",
    "            # gyro_curTime = gyro_sample['Creation_Time']\n",
    "            # print(acc_curTime, gyro_curTime, abs(acc_curTime - gyro_curTime))\n",
    "            if abs(acc_curTime - gyro_curTime) < 0.1:\n",
    "                # print('gen_new_row')\n",
    "                new_row = {}\n",
    "                new_row['Time'] = 0.5 * (acc_curTime + gyro_curTime)\n",
    "                new_row['labels'] = label\n",
    "                new_row['acc_x'] = user_acc_data.iloc[acc_idx]['x']\n",
    "                new_row['acc_y'] = user_acc_data.iloc[acc_idx]['y']\n",
    "                new_row['acc_z'] = user_acc_data.iloc[acc_idx]['z']\n",
    "                new_row['gyro_x'] = user_gyro_data.iloc[gyro_idx]['x']\n",
    "                new_row['gyro_y'] = user_gyro_data.iloc[gyro_idx]['y']\n",
    "                new_row['gyro_z'] = user_gyro_data.iloc[gyro_idx]['z']\n",
    "                \n",
    "                new_pd = new_pd.append(new_row, ignore_index=True)\n",
    "\n",
    "                acc_idx += 1\n",
    "                gyro_idx += 1\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                if acc_curTime - gyro_curTime >= 0.1:  # 10hz\n",
    "                    gyro_idx += 1\n",
    "                elif gyro_curTime - acc_curTime >= 0.1:\n",
    "                    acc_idx += 1\n",
    "                    pbar.update(1)\n",
    "    print(f'user_acc_{label}.shape: {user_acc_data.shape}')\n",
    "    print(f'user_gyro_{label}.shape: {user_gyro_data.shape}')\n",
    "    print(f'new_pd.shape: {new_pd.shape}')\n",
    "    # print(new_pd.head(), \"\\n\")\n",
    "    return new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec49c612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T16:41:20.705985Z",
     "iopub.status.busy": "2023-01-29T16:41:20.705656Z",
     "iopub.status.idle": "2023-01-29T16:41:20.723811Z",
     "shell.execute_reply": "2023-01-29T16:41:20.722933Z"
    },
    "papermill": {
     "duration": 0.023159,
     "end_time": "2023-01-29T16:41:20.725562",
     "exception": false,
     "start_time": "2023-01-29T16:41:20.702403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_window(time_series, width, step, order='F'):\n",
    "    w = np.hstack([time_series[i:1 + i - width or None:step] for i in range(0, width)])\n",
    "    result = w.reshape((int(len(w) / width), width), order='F')\n",
    "    if order == 'F':\n",
    "        return result\n",
    "    else:\n",
    "        return np.ascontiguousarray(result)\n",
    "\n",
    "def calc_normalization(data):\n",
    "    num_instances, num_time_steps, num_features = data.shape\n",
    "    data = np.reshape(data, (num_instances, -1))\n",
    "    scaler.fit(data)\n",
    "#     mean, std = (np.array([np.mean(x) for x in X_train], dtype=np.float32), np.array([np.std(x) for x in X_train], dtype=np.float32))\n",
    "    return scaler\n",
    "    \n",
    "def apply_normalization(data, scaler):\n",
    "#     scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = data.shape\n",
    "    data = np.reshape(data, (num_instances, -1))\n",
    "    norm_data = scaler.transform(data)\n",
    "#     debug_here()\n",
    "#     data = (data - mean) / (std + 1e-5)\n",
    "    norm_data[np.isnan(norm_data)] = 0\n",
    "    norm_data = np.reshape(norm_data, (num_instances, num_time_steps, num_features))\n",
    "    return norm_data\n",
    "\n",
    "def HHAR_data_grnerator(model_dats, model_name):\n",
    "    seq_length = 128  # 数据长度\n",
    "    shifting_step = 128  # overlap=0\n",
    "    channel_nums = 6  # 各个模态的通道数，每个采样结果的数据量\n",
    "    # acc data cleaning\n",
    "    acc_dat = model_dats[0]\n",
    "    acc_dat.dropna()\n",
    "    acc_dat['gt'] = acc_dat['gt'].astype('category')\n",
    "    acc_dat['labels']=acc_dat['gt'].cat.codes\n",
    "    acc_dat_refined = acc_dat.drop(columns=['Index', 'Arrival_Time', 'Model', 'Device' , 'gt'])\n",
    "\n",
    "    # gyro data cleaning\n",
    "    gyro_dat = model_dats[1]\n",
    "    gyro_dat.dropna()\n",
    "    gyro_dat['gt'] = gyro_dat['gt'].astype('category')\n",
    "    gyro_dat['labels']=gyro_dat['gt'].cat.codes\n",
    "    gyro_dat_refined = gyro_dat.drop(columns=['Index', 'Arrival_Time', 'Model', 'Device' , 'gt'])\n",
    "\n",
    "    # split user data\n",
    "    user_index = 0\n",
    "    for user_name, user_acc_data in tqdm(acc_dat_refined.groupby('User'), desc=\"Processing User\"):\n",
    "        # print(f\"user: {user_name}\")\n",
    "        data, labels = [], []\n",
    "        user_gyro_data = gyro_dat_refined[gyro_dat_refined['User']==user_name]\n",
    "\n",
    "        # 用户数据按照label进行处理\n",
    "        for label_name, user_acc_label_data in tqdm(user_acc_data.groupby('labels'), desc=\"Processing User label\"):\n",
    "            # user sensor pairing\n",
    "            if label_name == -1:\n",
    "                continue\n",
    "            user_gyro_label_data = user_gyro_data[user_gyro_data['labels']==label_name]\n",
    "            paired_data = sensor_pairing(user_acc_label_data, user_gyro_label_data, label=label_name)  # time, x,y,z,x,y,z, label\n",
    "            sliced_data = np.empty((int(paired_data.shape[0]/seq_length), seq_length, channel_nums))  # (N, 128, 6)\n",
    "\n",
    "            # 按找通道数据进行分割，添加到sliced_data\n",
    "            channl_idx = 0\n",
    "            for channel in paired_data[['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']]:\n",
    "                channel_data = paired_data[channel]\n",
    "                sliced_data[:,:,channl_idx] = sliding_window(channel_data.values, seq_length, shifting_step, 'T')\n",
    "                channl_idx += 1\n",
    "            \n",
    "            # append label data \n",
    "            data.append(sliced_data)\n",
    "            # gen labels\n",
    "            class_labels = np.empty(sliced_data.shape[0])\n",
    "            class_labels.fill(label_name)\n",
    "            labels.append(class_labels.astype(int))\n",
    "            \n",
    "        # data and labels for each users \n",
    "        array_user_data= np.concatenate(data, axis=0)\n",
    "        array_user_labels= np.concatenate(labels, axis=0)\n",
    "        \n",
    "        # Stratified train, validation, test split of the data \n",
    "        X_train, X_test, y_train, y_test = train_test_split(array_user_data, array_user_labels,  stratify=array_user_labels,  test_size=0.3,random_state=1)\n",
    "        # print(X_train.shape)\n",
    "        # print(y_train.shape)\n",
    "\n",
    "        # Data normalization \n",
    "        # Calculate mean and standard deviation based on train\n",
    "        scaler = calc_normalization(X_train)\n",
    "        \n",
    "        # Apply normalization \n",
    "        X_train = apply_normalization(X_train,scaler)\n",
    "#         X_val = apply_normalization(X_val,scaler)\n",
    "        X_test = apply_normalization(X_test,scaler)\n",
    "        \n",
    "        # prepare samples\n",
    "        train_data = {'samples':X_train, 'labels':y_train}\n",
    "#         val_data   = {'samples':X_val, 'labels':y_val}\n",
    "        test_data  = {'samples':X_test, 'labels':y_test}\n",
    "        \n",
    "        # save\n",
    "        os.makedirs(f'/kaggle/working/HHAR_data/{model_name}', exist_ok=True)\n",
    "        torch.save(train_data, f'/kaggle/working/HHAR_data/{model_name}/train_{user_index}.pt')\n",
    "#         torch.save(val_data,  f'HHAR_user_data/val_{user_name}.pt')\n",
    "        torch.save(test_data, f'/kaggle/working/HHAR_data/{model_name}/test_{user_index}.pt')\n",
    "\n",
    "        user_index+=1\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494fe323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T16:41:20.731729Z",
     "iopub.status.busy": "2023-01-29T16:41:20.731431Z",
     "iopub.status.idle": "2023-01-29T20:49:32.558652Z",
     "shell.execute_reply": "2023-01-29T20:49:32.554722Z"
    },
    "papermill": {
     "duration": 14891.834065,
     "end_time": "2023-01-29T20:49:32.562229",
     "exception": false,
     "start_time": "2023-01-29T16:41:20.728164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dats = split_Model_data(HHAR_phones_acc, HHAR_phones_gyro)\n",
    "HHAR_phones_acc = None\n",
    "HHAR_phones_gyro = None\n",
    "models = list(model_dats.keys())\n",
    "\n",
    "model_name = models[0]\n",
    "model_acc_gyro_dats = model_dats[model_name]\n",
    "# 节省内存\n",
    "model_dats = None\n",
    "\n",
    "HHAR_data_grnerator(model_dats=model_acc_gyro_dats, model_name=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14959.739451,
   "end_time": "2023-01-29T20:49:42.955194",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-29T16:40:23.215743",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
